{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6052db96-5cd5-4e34-8a15-c9bb1c56c3c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# From Social Science to Data Science: Preparing a Stack Exchange DataFrame\n",
    "\n",
    "**Author**: Bernie Hogan, bernie.hogan@oii.ox.ac.uk\n",
    "\n",
    "**Last Modification**: October 17, 2023\n",
    "\n",
    "Run this cell below for a one-click solution to get from any Stack Exchange archive to a raw XML file and DataFrame pickle that you can use with Chapters 10-13 of Hogan's _From Social Science to Data Science_. \n",
    "\n",
    "See bottom of file for change log. \n",
    "\n",
    "## To use\n",
    "\n",
    "Simply run the cell. The rest should be self-explanatory from the widgets (i.e. buttons) it produces. Please note that all required packages should have already been installed with anaconda, except possibly `py7zr` which should download and install automatically from `pip`. \n",
    "\n",
    "## Features\n",
    "\n",
    "This is script is very advanced compared to what you have seen in the book, but it shouldn't be completely intimidating. Compared to the code in Chapter 10, here are some interesting features that you might want to check out: \n",
    "\n",
    "- I use `ipywidgets` in order to create buttons instead of just running code. \n",
    "- I use `tqdm` to create a progress bar for the file. \n",
    "- I update the status of the processing during the work.\n",
    "- I use `ElementTree` instead of `json_normalise()`. It's more fussy, but it's faster.\n",
    "- I refactored the `cleanBody` and `cleanTags` so that it is only one call to BeautifulSoup.\n",
    "- I use `multiprocessing` in order to make the processing of text work a lot faster on modern multi-core processors. \n",
    "- I have options so that you can keep the original 7zip archive if you wish. \n",
    "- The methods should all have docstrings and type hinting. \n",
    "- ChatGPT gave lots of useful pointers and tips, but no large code snippets.\n",
    "\n",
    "## Future extensions \n",
    "\n",
    "This script hardcodes for several things which might be made more general. \n",
    "- The original archive at \"https://archive.org/download/stackexchange\"\n",
    "- The default download directory (in a sibling folder to the parent of this file under 'data')\n",
    "\n",
    "It would be useful in the future to:\n",
    "- Ensure compatibility when storing data on Google Drive for Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760b4c8-caa9-4c03-8632-b5da443611e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1a79080e4141c19705eee5473bc4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 60] Operation timed out>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/urllib/request.py:1344\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1344\u001b[39m     \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTransfer-encoding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/http/client.py:1338\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1337\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/http/client.py:1384\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1383\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/http/client.py:1333\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1333\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/http/client.py:1093\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1096\u001b[39m \n\u001b[32m   1097\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/http/client.py:1037\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/http/client.py:1472\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1470\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mConnect to a host on a given (SSL) port.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1472\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tunnel_host:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/http/client.py:1003\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1002\u001b[39m sys.audit(\u001b[33m\"\u001b[39m\u001b[33mhttp.client.connect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m.port)\n\u001b[32m-> \u001b[39m\u001b[32m1003\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/socket.py:865\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_errors:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m    866\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ExceptionGroup(\u001b[33m\"\u001b[39m\u001b[33mcreate_connection failed\u001b[39m\u001b[33m\"\u001b[39m, exceptions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/socket.py:850\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    849\u001b[39m     sock.bind(source_address)\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mTimeoutError\u001b[39m: [Errno 60] Operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mURLError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 542\u001b[39m\n\u001b[32m    538\u001b[39m     se_download.on_click(on_click_worker)\n\u001b[32m    540\u001b[39m     display(vbox)\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m \u001b[43mdownload_stackexchange_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 386\u001b[39m, in \u001b[36mdownload_stackexchange_archive\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m se_intro:\n\u001b[32m    384\u001b[39m     display(Markdown(\u001b[33m\"\u001b[39m\u001b[33mLoading Stack Exchange list...\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m df = process_archive_table(\u001b[43mdownload_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mURL\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    388\u001b[39m se_dropdown = iw.Dropdown(\n\u001b[32m    389\u001b[39m     options= [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx[\u001b[32m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \n\u001b[32m    390\u001b[39m               \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(df.index, df[\u001b[33m\"\u001b[39m\u001b[33mstack_name\u001b[39m\u001b[33m\"\u001b[39m], df[\u001b[33m\"\u001b[39m\u001b[33mSize\u001b[39m\u001b[33m\"\u001b[39m])],\n\u001b[32m    391\u001b[39m     value=\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[32m    392\u001b[39m     description=\u001b[33m\"\u001b[39m\u001b[33mArchive:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    394\u001b[39m se_dropdown.layout.width = \u001b[33m'\u001b[39m\u001b[33m50\u001b[39m\u001b[33m%\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mdownload_table\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload_table\u001b[39m(url = \u001b[33m\"\u001b[39m\u001b[33mhttps://archive.org/download/stackexchange\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     27\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m    Downloads URL and parses for tables, and returns first table, which is a directory listing of all Stack Exchanges. \u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m \u001b[33;03m    pd.DataFrame: Table as unformatted DataFrame.\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     result = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GroupPres_SDS/.venv/lib/python3.12/site-packages/pandas/io/html.py:1240\u001b[39m, in \u001b[36mread_html\u001b[39m\u001b[34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[39m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   1225\u001b[39m     [\n\u001b[32m   1226\u001b[39m         is_file_like(io),\n\u001b[32m   (...)\u001b[39m\u001b[32m   1230\u001b[39m     ]\n\u001b[32m   1231\u001b[39m ):\n\u001b[32m   1232\u001b[39m     warnings.warn(\n\u001b[32m   1233\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing literal html to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mread_html\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1234\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future version. To read from a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GroupPres_SDS/.venv/lib/python3.12/site-packages/pandas/io/html.py:983\u001b[39m, in \u001b[36m_parse\u001b[39m\u001b[34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[39m\n\u001b[32m    972\u001b[39m p = parser(\n\u001b[32m    973\u001b[39m     io,\n\u001b[32m    974\u001b[39m     compiled_match,\n\u001b[32m   (...)\u001b[39m\u001b[32m    979\u001b[39m     storage_options,\n\u001b[32m    980\u001b[39m )\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     tables = \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[32m    985\u001b[39m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[32m    986\u001b[39m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[33m\"\u001b[39m\u001b[33mseekable\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io.seekable():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GroupPres_SDS/.venv/lib/python3.12/site-packages/pandas/io/html.py:249\u001b[39m, in \u001b[36m_HtmlFrameParser.parse_tables\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    242\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[33;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[32m    244\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m \u001b[33;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     tables = \u001b[38;5;28mself\u001b[39m._parse_tables(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.match, \u001b[38;5;28mself\u001b[39m.attrs)\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GroupPres_SDS/.venv/lib/python3.12/site-packages/pandas/io/html.py:806\u001b[39m, in \u001b[36m_LxmlFrameParser._build_doc\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    804\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    805\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    807\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(r, \u001b[33m\"\u001b[39m\u001b[33mtext_content\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GroupPres_SDS/.venv/lib/python3.12/site-packages/pandas/io/html.py:785\u001b[39m, in \u001b[36m_LxmlFrameParser._build_doc\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    784\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_url(\u001b[38;5;28mself\u001b[39m.io):\n\u001b[32m--> \u001b[39m\u001b[32m785\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    788\u001b[39m             r = parse(f.handle, parser=parser)\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    790\u001b[39m         \u001b[38;5;66;03m# try to parse the input in the simplest way\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GroupPres_SDS/.venv/lib/python3.12/site-packages/pandas/io/common.py:728\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    725\u001b[39m     codecs.lookup_error(errors)\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ioargs = \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m handle = ioargs.filepath_or_buffer\n\u001b[32m    737\u001b[39m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GroupPres_SDS/.venv/lib/python3.12/site-packages/pandas/io/common.py:384\u001b[39m, in \u001b[36m_get_filepath_or_buffer\u001b[39m\u001b[34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[32m    383\u001b[39m req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[32m    385\u001b[39m     content_encoding = req.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Encoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GroupPres_SDS/.venv/lib/python3.12/site-packages/pandas/io/common.py:289\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[33;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03mthe stdlib.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrequest\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/urllib/request.py:215\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/urllib/request.py:515\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    512\u001b[39m     req = meth(req)\n\u001b[32m    514\u001b[39m sys.audit(\u001b[33m'\u001b[39m\u001b[33murllib.Request\u001b[39m\u001b[33m'\u001b[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[32m    518\u001b[39m meth_name = protocol+\u001b[33m\"\u001b[39m\u001b[33m_response\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/urllib/request.py:532\u001b[39m, in \u001b[36mOpenerDirector._open\u001b[39m\u001b[34m(self, req, data)\u001b[39m\n\u001b[32m    529\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    531\u001b[39m protocol = req.type\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/urllib/request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    491\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/urllib/request.py:1392\u001b[39m, in \u001b[36mHTTPSHandler.https_open\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/urllib/request.py:1347\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1344\u001b[39m         h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[32m   1345\u001b[39m                   encode_chunked=req.has_header(\u001b[33m'\u001b[39m\u001b[33mTransfer-encoding\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m   1346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[32m   1348\u001b[39m     r = h.getresponse()\n\u001b[32m   1349\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[31mURLError\u001b[39m: <urlopen error [Errno 60] Operation timed out>"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import ipywidgets as iw\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Markdown\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import bs4 \n",
    "import xml.etree.ElementTree as ET\n",
    "import multiprocessing\n",
    "\n",
    "# This might not be installed. It shouldn't cause trouble to live install\n",
    "try: \n",
    "    import py7zr\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install py7zr\n",
    "    import py7zr\n",
    "\n",
    "\n",
    "URL = \"https://archive.org/download/stackexchange\"\n",
    "DOWNLOAD_FOLDER = Path().cwd().parent / \"data\"\n",
    "\n",
    "def download_table(url = \"https://archive.org/download/stackexchange\"):\n",
    "    \"\"\"\n",
    "    Downloads URL and parses for tables, and returns first table, which is a directory listing of all Stack Exchanges. \n",
    "\n",
    "    Parameters: \n",
    "    url (str): A path to the directory listing.\n",
    "\n",
    "    Returns: \n",
    "    pd.DataFrame: Table as unformatted DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    result = pd.read_html(url)\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "def process_archive_table(df: pd.DataFrame) -> pd.DataFrame: \n",
    "    \"\"\"\n",
    "    Processes the text from an Internet Archive listing of Stack Exchanges.\n",
    "\n",
    "    Returns: \n",
    "    pd.DataFrame: A DataFrame with new columns for the name of the Exchange.\n",
    "             Excludes StackOverflow due to size limitations and meta.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract valid stacks from directory listing\n",
    "    df['stack_file_name'] = (\n",
    "        df[\"Name\"].str\n",
    "        .extract( r\"(?P<stack_name>\\w*\\.stackexchange\\.com.7z)\"))\n",
    "\n",
    "    # Filter to valid stacks\n",
    "    df = (\n",
    "        df[df['stack_file_name']\n",
    "        .map(lambda x: isinstance(x, str) and not \"meta\" in x)]\n",
    "        .copy())\n",
    "\n",
    "    # Create nice column\n",
    "    df['stack_name'] = (\n",
    "        df['stack_file_name']\n",
    "        .map(lambda x: x.split(\".\")[0]))\n",
    "\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def download_archive(\n",
    "    url: str, dest: str, output_widget: iw.Output, **kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Function to download a Stack Exchange archive. It takes in an ipyoutput widget in order to use to update download progress.\n",
    "\n",
    "    Returns:\n",
    "    Path: A path object pointing to the downloaded file\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(dest, str):\n",
    "        dest = Path(dest)\n",
    "\n",
    "    if not dest.exists():\n",
    "        with output_widget: \n",
    "            display(Markdown(f\"Please create a folder at the location:\\n {download_folder}\\n\\nNote: You can use relative paths with `..` to mean one folder up. So if you are in a parallel folder to your data folder, `../data` (or `..\\\\data` on windows) will be what you need. Otherwise, you can always write the full path name such as:\\n{download_folder.resolve()} .\"))\n",
    "        return False\n",
    "    else:\n",
    "        DOWNLOAD_FOLDER = dest.resolve()\n",
    "\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    dest_file = dest / filename\n",
    "\n",
    "    resp = requests.get(url, stream=True)\n",
    "    total = int(resp.headers.get('content-length', 0))\n",
    "\n",
    "    if total <= 0: \n",
    "        with output_widget:\n",
    "            print('The file size was 0. Is the URL correct?')\n",
    "            return False\n",
    "\n",
    "    with output_widget: \n",
    "        with open(dest_file, 'wb') as file, tqdm(\n",
    "        total=total,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for data in resp.iter_content(chunk_size=1024):\n",
    "                size = file.write(data)\n",
    "                bar.update(size)\n",
    "\n",
    "    if 'extract' in kwargs and kwargs['extract']:\n",
    "        if 'keep7z' in kwargs:\n",
    "            archive_folder = extract_archive(\n",
    "                dest_file,keep7z=kwargs['keep7z']) \n",
    "        return archive_folder\n",
    "    else:\n",
    "        return dest_file\n",
    "\n",
    "\n",
    "def extract_archive(archive_path: Path, **kwargs) -> Path:\n",
    "    \"\"\"\n",
    "    Function to extract a StackExchange archive from 7z.\n",
    "    The archive is placed in a folder of the same name. \n",
    "\n",
    "    Parameters:\n",
    "    keep7z (Boolean): Keep the original 7z file. It is deleted by default.\n",
    "\n",
    "    Returns:\n",
    "    str: the folder path containing the XML files.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(archive_path, str):\n",
    "        archive_path = Path(archive_path)\n",
    "\n",
    "    file_name = archive_path.name\n",
    "    folder_name = \".\".join(archive_path.name.split(\".\")[:-1])\n",
    "    archive_folder = archive_path.parent / folder_name\n",
    "    archive_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    with py7zr.SevenZipFile(archive_path, 'r') as archive:\n",
    "        archive.extractall(archive_folder)\n",
    "\n",
    "    if 'keep7z' in kwargs and kwargs['keep7z']:\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            os.remove(archive_path)\n",
    "        except:\n",
    "            print(\"The original 7z could not be deleted\")\n",
    "\n",
    "    return archive_folder\n",
    "\n",
    "from collections.abc import Sequence \n",
    "\n",
    "def _seq_but_not_str(obj):\n",
    "    return isinstance(obj, Sequence) and not isinstance(obj, (str, bytes, bytearray))\n",
    "\n",
    "\n",
    "def convertXMLtoDataFrame(xml_path: Path, cols = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parser to process an XML file into a DataFrame. Presently assumes it is\n",
    "    a StackExchange file with the structure of Posts.xml\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Unprocessed DataFrame directory from XML data.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(xml_path, str):\n",
    "        xml_path = Path(xml_path)\n",
    "\n",
    "    assert xml_path.exists(), \"The file path is not valid\"\n",
    "    \n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract the data from the XML elements\n",
    "\n",
    "    if cols: \n",
    "        rows = [{var: row.get(var) for var in cols} \n",
    "                 for row in root.findall('row')]\n",
    "    else: \n",
    "        rows = [{attr: row.get(attr) for attr in row.attrib} \n",
    "                for row in root.findall('row')]\n",
    "\n",
    "    # Create a DataFrame from the rows\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def is_numeric(series):\n",
    "    try:\n",
    "        pd.to_numeric(series, errors='raise')\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "    except pd.errors.OutOfBoundsDatetime:\n",
    "        return False\n",
    "\n",
    "def convertColumns(df: pd.DataFrame, \n",
    "                   exclude_id_cols = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Formatter function to change column types to more meaningful types than the default `str` that comes from the XML parsing\n",
    "\n",
    "    Returns: \n",
    "    pd.DataFrame: A DataFrame with the typed columns \n",
    "    \"\"\"\n",
    "\n",
    "    cols = df.columns\n",
    "    \n",
    "    if exclude_id_cols:\n",
    "        cols = [c for c in cols if not c.endswith(\"Id\")]\n",
    "    \n",
    "    for col in cols: \n",
    "        if col.endswith('Date'):\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')        \n",
    "        elif is_numeric(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "def parseHTML(text: str, col_name: str, **kwargs) -> dict:\n",
    "    \"\"\"\n",
    "    Parser for HTML comment and returns a tuple with comment text (or None) and list of links (or None)\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary with data of form {\"cleantext\": str, \"links\": list}.\n",
    "    \"\"\"\n",
    "\n",
    "    ret_dict = {}\n",
    "\n",
    "    #It gets moody because some comments are very terse.\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\") \n",
    "        if type(text) == float:\n",
    "            if 'bodytext' in kwargs and kwargs['bodytext']: \n",
    "                ret_dict[f'{col_name}Text'] = pd.NA\n",
    "            if 'links' in kwargs and kwargs['links']: \n",
    "                ret_dict[f'{col_name}URLs'] = pd.NA\n",
    "        else:\n",
    "            try: \n",
    "                soup = bs4.BeautifulSoup(text, 'lxml')\n",
    "                if 'bodytext' in kwargs and kwargs['bodytext']:\n",
    "                    ret_dict[f'{col_name}Text'] = soup.text.replace(\"\\n\",\" \")\n",
    "\n",
    "                if 'links' in kwargs and kwargs['links']:\n",
    "                    ret_dict[f'{col_name}URLs'] = [x['href'] for x in soup.find_all('a')\n",
    "                        if 'href' in x.attrs and \"://\" in x.get('href')]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(type(e), e)\n",
    "\n",
    "    return ret_dict\n",
    "\n",
    "def cleanBody(body_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Worker that manages the pooled processing of many Body rows to clean.\n",
    "    \"\"\"\n",
    "    col_name = body_series.name\n",
    "\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        # Use apply to process each row in parallel\n",
    "        results = body_series.map(\n",
    "            lambda row: parseHTML(\n",
    "                row,bodytext=True,links=True,col_name=col_name))\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        results.tolist(), columns=[f'{col_name}Text', f'{col_name}URLs'])\n",
    "\n",
    "def splitTags(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Formatter that parses text of form `<tag1><tag2><...>` to return a list of `['tag1', 'tag2',...]`\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A two-column DataFrame with the same ID as original for merging.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(text) != str or len(text) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        return text[1:-1].split(\"><\")\n",
    "\n",
    "def cleanTags(tag_text_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Worker that manages the pooled processing of many Tags rows to clean.\n",
    "    \"\"\"\n",
    "\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        results = tag_text_series.map(splitTags)\n",
    "    \n",
    "    new_df = pd.DataFrame(results)\n",
    "    new_df.columns = ['TagsList']\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def df_to_file(df, xml_path, file_type=\"parquet\", **kwargs):\n",
    "\n",
    "    msg = \"\"\n",
    "    \n",
    "    if file_type == \"parquet\": \n",
    "        import pyarrow.parquet as pq\n",
    "        import pyarrow as pa\n",
    "        \n",
    "        out_path = xml_path.parent / f\"{xml_path.stem}.{file_type}\"\n",
    "        \n",
    "        pq.write_table(pa.Table.from_pandas(df),out_path)\n",
    "        msg += f\"The file is available here: <br> `{str(out_path)}`\"\n",
    "        msg +=  \"<br> You can load this file and call `df` with <br> `df = \"\n",
    "        msg += f\" pq.read_table('{str(out_path)}').to_pandas()`<br>\"\n",
    "\n",
    "    \n",
    "    elif file_type == \"feather\": \n",
    "        out_path = xml_path.parent / f\"{xml_path.stem}.{file_type}\"\n",
    "        df.to_feather(out_path)\n",
    "        msg += f\"The file is available here: <br> `{str(out_path)}`\" \n",
    "        msg += f\"<br> You can load this file and call `df` \"\n",
    "        msg += f\"with <br> `df = pd.read_feather('{str(out_path)}')`\"\n",
    "          \n",
    "    elif file_type == \"pickle\":\n",
    "        import pickle\n",
    "        out_path = xml_path.parent / f\"{xml_path.stem}.{file_type}\"\n",
    "        df.to_pickle(out_path)\n",
    "        msg += f\"The file is available here: <br> `{str(out_path)}`\"\n",
    "        msg += f\"<br> You can load this file and call `df` \"\n",
    "        msg += f\"with <br> `df = pd.read_pickle('{str(out_path)}')`<br>\"\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return msg\n",
    "    \n",
    "def makeDataFrame(xml_path, file_out=\"feather\", **kwargs):\n",
    "    \"\"\"\n",
    "    Worker that steps through key processing from XML to DataFrame. \n",
    "\n",
    "    Parameters:\n",
    "        extract_body_text (Boolean): \n",
    "            Creates new columns for body text, the first has no HTML, \n",
    "            the second is a list of links in body text.\n",
    "        extract_tags (Boolean): Creates a new column for tags as a list \n",
    "            rather than a string of the form <tag1><tag2>\n",
    "        file_out (str): Returns the DataFrame in this format of \"pickle\", \n",
    "            \"feather\", \"csv\". Default is feather. CSV is not implemented yet.\n",
    "    \n",
    "    Returns:\n",
    "        str: A file path to the DataFrame that has been saved.\n",
    "    \"\"\"\n",
    "\n",
    "    df = convertXMLtoDataFrame(xml_path)\n",
    "    df = convertColumns(df)\n",
    "    concat_cols = [df]\n",
    "\n",
    "    if 'extract_body_text' in kwargs:\n",
    "        cols = kwargs['extract_body_text']\n",
    "        if type(cols) == str: \n",
    "            cols = [cols]\n",
    "            \n",
    "        if type(cols) == list:\n",
    "            for col in cols: \n",
    "                concat_cols.append(cleanBody(df[col]))\n",
    "                \n",
    "    if 'extract_tags' in kwargs and kwargs['extract_tags']:\n",
    "        concat_cols.append(cleanTags(df['Tags']))\n",
    "    \n",
    "    df = pd.concat(\n",
    "        concat_cols,\n",
    "        axis = 1)\n",
    "\n",
    "    return df_to_file(df,xml_path,file_type=file_out)\n",
    "\n",
    "def display_intro():\n",
    "    se_intro = iw.Output()\n",
    "    display(se_intro)\n",
    "\n",
    "\n",
    "def download_stackexchange_archive():\n",
    "    \"\"\"\n",
    "    Main worker function. Handles the downloading and population of the initial list, displaying options, and reporting outcome to user. \n",
    "    \"\"\"\n",
    "\n",
    "    se_intro = iw.Output()\n",
    "    display(se_intro)\n",
    "\n",
    "    with se_intro:\n",
    "        display(Markdown(\"Loading Stack Exchange list...\"))\n",
    "\n",
    "    df = process_archive_table(download_table(URL))\n",
    "\n",
    "    se_dropdown = iw.Dropdown(\n",
    "        options= [f\"{x[0]}. {x[1]} - {x[2]}\" \n",
    "                  for x in zip(df.index, df[\"stack_name\"], df[\"Size\"])],\n",
    "        value=None, \n",
    "        description=\"Archive:\")\n",
    "\n",
    "    se_dropdown.layout.width = '50%'\n",
    "\n",
    "    se_intro.clear_output()\n",
    "    with se_intro:\n",
    "        display(Markdown(\"Please select a Stack Exchange to process (the book uses 'movies')\"))\n",
    "\n",
    "    se_progress = iw.Output()\n",
    "\n",
    "    se_download_path = iw.Text(\n",
    "        description='Folder:',\n",
    "        value=str(DOWNLOAD_FOLDER))\n",
    "    \n",
    "    se_download_path.layout.width = '50%'\n",
    "\n",
    "    se_download = iw.Button(description=\"Download\", disabled=True)\n",
    "    \n",
    "    se_check_keep7z = iw.Checkbox(\n",
    "        description='Keep downloaded 7z file on disk', value=True) \n",
    "    se_check_useXML = iw.Checkbox(\n",
    "        description='Use existing XML file if found', value=True) \n",
    "    se_process_posts = iw.Checkbox(\n",
    "        description='Process Posts.xml to DataFrame', value=True) \n",
    "    se_process_users = iw.Checkbox(\n",
    "        description='Process Users.xml to DataFrame', value=False) \n",
    "    se_check_body_cols = iw.Checkbox(\n",
    "        description='Insert columns of text stripped of HTML and URLs',\n",
    "        value=True) \n",
    "    se_check_tags_list = iw.Checkbox(\n",
    "        description='Process Tags column to list of tags',\n",
    "        value=True) \n",
    "\n",
    "    se_file_type = iw.Dropdown(\n",
    "        options= [\"parquet\",\"feather\",\"pickle\"],\n",
    "        value=\"feather\", \n",
    "        description=\"File Type for Export:\")\n",
    "\n",
    "    se_file_type.layout.width = '50%'\n",
    "\n",
    "    \n",
    "    def handle_dropdown_change(change):\n",
    "        '''Internal worker to enable button state once archive selected'''\n",
    "\n",
    "        selected_option = change.new\n",
    "\n",
    "        url_index = int(selected_option.split(\".\")[0])\n",
    "\n",
    "        se_download.disabled = False\n",
    "        se_progress.clear_output()\n",
    "\n",
    "        with se_progress: \n",
    "            display(Markdown(\n",
    "            f\"You have selected: `{ df.loc[url_index]['stack_file_name']}`.\" +\n",
    "            f\"\\nThis option is `{df.loc[url_index]['Size']}` in size. \" +\n",
    "             \"Click to download.\"))\n",
    "\n",
    "    def handle_filetype_change(change):\n",
    "        se_download.disabled = False\n",
    "        se_progress.clear_output()\n",
    "        \n",
    "    def on_click_worker(_):\n",
    "        '''Internal worker to manage the process of downloading and processing\n",
    "        once a button has been pressed'''\n",
    "\n",
    "        se_download.disabled = True\n",
    "\n",
    "        with se_progress:\n",
    "\n",
    "            archive_name = df.loc[\n",
    "                int(se_dropdown.value.split(\".\")[0])][\"stack_file_name\"]\n",
    "            archive_remote_path = URL + '/' + archive_name\n",
    "            archive_dest = Path(se_download_path.value).resolve()\n",
    "            \n",
    "            dl_posts = se_process_posts.value\n",
    "            dl_users = se_process_users.value\n",
    "            \n",
    "            if se_check_useXML.value:\n",
    "                \n",
    "                archive_folder = archive_dest / archive_name.split(\".7z\")[0]\n",
    "                \n",
    "                if se_process_posts.value:\n",
    "                    if (archive_folder / \"Posts.xml\").exists():\n",
    "                        dl_posts = False\n",
    "                        display(Markdown(\"Using existing Posts XML file\"))\n",
    "                if se_process_users.value:\n",
    "                    if (archive_folder / \"Users.xml\").exists():\n",
    "                        dl_users = False\n",
    "                        display(Markdown(\"Using existing Users XML file\"))\n",
    "                    \n",
    "            if dl_posts or dl_users:\n",
    "                archive_folder = download_archive(\n",
    "                url = archive_remote_path, \n",
    "                dest = archive_dest, \n",
    "                output_widget = se_progress,\n",
    "                extract = True,\n",
    "                keep7z = se_check_keep7z.value)\n",
    "\n",
    "                display(Markdown(\"Processing downloaded file...\"))\n",
    "\n",
    "            status = \"\"\n",
    "        \n",
    "            if se_process_posts.value:\n",
    "                \n",
    "                body_cols = False\n",
    "                if se_check_body_cols.value:\n",
    "                    body_cols=\"Body\"\n",
    "\n",
    "                status += makeDataFrame(\n",
    "                    xml_path = archive_folder / (\"Posts.xml\"),\n",
    "                    file_out = se_file_type.value,\n",
    "                    extract_body_text = body_cols,\n",
    "                    extract_tags = se_check_tags_list.value)\n",
    "\n",
    "            if se_process_users.value:\n",
    "                body_cols = False\n",
    "                if se_check_body_cols.value:\n",
    "                    body_cols=\"AboutMe\" \n",
    "    \n",
    "                status += makeDataFrame(\n",
    "                    xml_path = archive_folder / (\"Users.xml\"),\n",
    "                    file_out = se_file_type.value,\n",
    "                    extract_body_text = body_cols)\n",
    "\n",
    "            display(Markdown(status))\n",
    "\n",
    "    settings_group = iw.VBox([\n",
    "        se_check_keep7z,\n",
    "        se_check_useXML,\n",
    "        se_check_body_cols,\n",
    "        se_check_tags_list,\n",
    "        se_process_posts,\n",
    "        se_process_users])\n",
    "\n",
    "    vbox = iw.VBox([\n",
    "        se_dropdown,\n",
    "        se_file_type,\n",
    "        se_download_path, \n",
    "        settings_group,\n",
    "        se_download, \n",
    "        se_progress])\n",
    "\n",
    "    se_dropdown.observe(handle_dropdown_change, names='value')\n",
    "    se_file_type.observe(handle_filetype_change)\n",
    "\n",
    "    \n",
    "    se_download.on_click(on_click_worker)\n",
    "\n",
    "    display(vbox)\n",
    "\n",
    "download_stackexchange_archive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016d82a0-ad33-4554-856e-0a04c3caabfb",
   "metadata": {},
   "source": [
    "## Change log \n",
    "\n",
    "## Update V1.2. October 19\n",
    "\n",
    "Better file management! If the folder with XML files has already been downloaded you can simply run download and it will process the files in there. This is handy if you want to add a Users.xml -> dataframe without redownloading everything. It is also handy if you want to have the files in a different format\n",
    "\n",
    "New format: parquet, default format remains `feather` as a faster i/o with higher compression and less import overhead. \n",
    "\n",
    "## Update V1.1. October 17 \n",
    "\n",
    "There is a breaking change in this code as I adopt a more consistent naming convention. `ListTags` is now `TagsLists` and `CleanBody` is now `BodyText` and `BodyURLs` in the case of Posts.xml. \n",
    "\n",
    "Version 1.1: This version now allows you to download either the `Posts.xml` or `Users.xml` which will be downloaded. The default export version is feather. Anaconda comes with the default packages but if you need to install, it would be `!pip install feather`. Parquet and pickle options are also available for backwards compatibility.\n",
    "\n",
    "Bug fixes: The export text now is quoted. The column types now enforce Id = Str and do not make int data as floats. The clean HTML code now also removes `\\n`. Code is a little more refactored in the XML parser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GroupPres_SDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
